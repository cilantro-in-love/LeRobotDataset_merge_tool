"""
å®Œæ•´åˆå¹¶å¤šä¸ª LeRobot æ•°æ®é›†çš„è„šæœ¬ï¼ˆåŒ…æ‹¬æ›´æ–° parquet æ–‡ä»¶å†…å®¹ï¼‰
"""
import json
import shutil
import pandas as pd
from pathlib import Path
from lerobot.datasets.lerobot_dataset import LeRobotDataset

def merge_lerobot_datasets(
    source_paths: list[str],
    target_repo_id: str,
    target_root: str,
):
    """
    å®Œæ•´åˆå¹¶å¤šä¸ª LeRobot æ•°æ®é›†ï¼ˆåŒ…æ‹¬æ›´æ–°æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼‰
    
    Args:
        source_paths: æºæ•°æ®é›†çš„æœ¬åœ°è·¯å¾„åˆ—è¡¨
        target_repo_id: ç›®æ ‡æ•°æ®é›†çš„ repo_id
        target_root: ç›®æ ‡æ•°æ®é›†çš„æœ¬åœ°æ ¹è·¯å¾„
    """
    # 1. åŠ è½½æ‰€æœ‰æºæ•°æ®é›†
    print("ğŸ“š åŠ è½½æºæ•°æ®é›†...")
    datasets = [LeRobotDataset(path) for path in source_paths]
    
    # 2. éªŒè¯æ•°æ®é›†å…¼å®¹æ€§
    print("âœ… éªŒè¯æ•°æ®é›†å…¼å®¹æ€§...")
    base_ds = datasets[0]
    for i, ds in enumerate(datasets[1:], 1):
        if ds.fps != base_ds.fps:
            raise ValueError(f"æ•°æ®é›† {i} çš„ fps ({ds.fps}) ä¸ç¬¬ä¸€ä¸ªæ•°æ®é›† ({base_ds.fps}) ä¸åŒ¹é…")
        if ds.meta.robot_type != base_ds.meta.robot_type:
            print(f"âš ï¸  è­¦å‘Š: æ•°æ®é›† {i} çš„ robot_type ä¸åŒ")
    
    # 3. åˆ›å»ºç›®æ ‡ç›®å½•
    target_path = Path(target_root)
    if target_path.exists():
        print(f"âš ï¸  ç›®æ ‡è·¯å¾„ {target_path} å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–...")
        shutil.rmtree(target_path)
    target_path.mkdir(parents=True)
    
    # 4. å‡†å¤‡åˆå¹¶çš„å…ƒæ•°æ®
    print("ğŸ“ å‡†å¤‡å…ƒæ•°æ®...")
    merged_info = base_ds.meta.info.copy()
    merged_episodes = {}
    merged_episodes_stats = {}
    merged_tasks = {}
    merged_task_to_task_index = {}
    
    total_episodes = 0
    total_frames = 0
    episode_offset = 0
    frame_offset = 0
    
    # 5. é€ä¸ªå¤„ç†æ¯ä¸ªæ•°æ®é›†
    for ds_idx, ds in enumerate(datasets):
        print(f"\nğŸ“¦ å¤„ç†æ•°æ®é›† {ds_idx + 1}/{len(datasets)}: {ds.repo_id}")
        
        # åˆå¹¶ tasks
        for task_idx, task in ds.meta.tasks.items():
            if task not in merged_task_to_task_index:
                new_task_idx = len(merged_tasks)
                merged_tasks[new_task_idx] = task
                merged_task_to_task_index[task] = new_task_idx
        
        # å¤„ç†æ¯ä¸ª episode
        for ep_idx in range(ds.meta.total_episodes):
            new_ep_idx = episode_offset + ep_idx
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šè¯»å–å¹¶æ›´æ–° parquet æ–‡ä»¶å†…å®¹
            src_data = ds.root / ds.meta.get_data_file_path(ep_idx)
            dst_data = target_path / base_ds.meta.get_data_file_path(new_ep_idx)
            dst_data.parent.mkdir(parents=True, exist_ok=True)
            
            # è¯»å– parquet æ–‡ä»¶
            df = pd.read_parquet(src_data)
            ep_length = len(df)
            
            # æ›´æ–° episode_index åˆ—
            df['episode_index'] = new_ep_idx
            
            # æ›´æ–° index åˆ—ï¼ˆå…¨å±€å¸§ç´¢å¼•ï¼‰
            df['index'] = range(frame_offset, frame_offset + ep_length)
            
            # ä¿å­˜æ›´æ–°åçš„ parquet æ–‡ä»¶
            df.to_parquet(dst_data, index=False)
            
            # å¤åˆ¶è§†é¢‘æ–‡ä»¶
            if ds.meta.video_keys:
                for vid_key in ds.meta.video_keys:
                    src_video = ds.root / ds.meta.get_video_file_path(ep_idx, vid_key)
                    if src_video.exists():
                        dst_video = target_path / base_ds.meta.get_video_file_path(new_ep_idx, vid_key)
                        dst_video.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(src_video, dst_video)
            
            # æ›´æ–° episodes å…ƒæ•°æ®
            ep_dict = ds.meta.episodes[ep_idx].copy()
            ep_dict["episode_index"] = new_ep_idx
            merged_episodes[new_ep_idx] = ep_dict
            
            # æ›´æ–° episodes_stats
            ep_stats = ds.meta.episodes_stats[ep_idx].copy()
            
            # æ›´æ–° stats ä¸­çš„ episode_index å­—æ®µ
            if "episode_index" in ep_stats:
                ep_stats["episode_index"]["min"] = [new_ep_idx]
                ep_stats["episode_index"]["max"] = [new_ep_idx]
                ep_stats["episode_index"]["mean"] = [float(new_ep_idx)]
            
            # æ›´æ–° stats ä¸­çš„ index å­—æ®µ
            if "index" in ep_stats:
                new_min = frame_offset
                new_max = frame_offset + ep_length - 1
                
                ep_stats["index"]["min"] = [new_min]
                ep_stats["index"]["max"] = [new_max]
                ep_stats["index"]["mean"] = [(new_min + new_max) / 2.0]
                # std éœ€è¦é‡æ–°è®¡ç®—
                import numpy as np
                ep_stats["index"]["std"] = [np.std(np.arange(new_min, new_max + 1)).item()]
            
            merged_episodes_stats[new_ep_idx] = ep_stats
            
            total_frames += ep_length
            frame_offset += ep_length
            
            print(f"  âœ“ Episode {ep_idx} -> {new_ep_idx} (frames {new_min}-{new_max}, {ep_length} frames)")
        
        episode_offset += ds.meta.total_episodes
        total_episodes += ds.meta.total_episodes
    
    # 6. æ›´æ–°å¹¶ä¿å­˜ info.json
    print("\nğŸ’¾ ä¿å­˜å…ƒæ•°æ®...")
    merged_info["total_episodes"] = total_episodes
    merged_info["total_frames"] = total_frames
    merged_info["total_tasks"] = len(merged_tasks)
    merged_info["total_chunks"] = (total_episodes - 1) // merged_info["chunks_size"] + 1
    merged_info["splits"] = {"train": f"0:{total_episodes}"}
    if merged_info.get("total_videos"):
        merged_info["total_videos"] = total_episodes * len(base_ds.meta.video_keys)
    
    (target_path / "meta").mkdir(exist_ok=True)
    with open(target_path / "meta" / "info.json", "w") as f:
        json.dump(merged_info, f, indent=2)
    
    # 7. ä¿å­˜ tasks.jsonl
    import jsonlines
    with jsonlines.open(target_path / "meta" / "tasks.jsonl", "w") as writer:
        for task_idx, task in merged_tasks.items():
            writer.write({"task_index": task_idx, "task": task})
    
    # 8. ä¿å­˜ episodes.jsonl
    with jsonlines.open(target_path / "meta" / "episodes.jsonl", "w") as writer:
        for ep_idx in sorted(merged_episodes.keys()):
            writer.write(merged_episodes[ep_idx])
    
    # 9. ä¿å­˜ episodes_stats.jsonl
    def serialize_stats(stats):
        """é€’å½’åºåˆ—åŒ– numpy æ•°ç»„ä¸ºåˆ—è¡¨"""
        import numpy as np
        if isinstance(stats, dict):
            return {k: serialize_stats(v) for k, v in stats.items()}
        elif isinstance(stats, (np.ndarray, list)):
            return np.array(stats).tolist()
        elif isinstance(stats, np.generic):
            return stats.item()
        return stats
    
    with jsonlines.open(target_path / "meta" / "episodes_stats.jsonl", "w") as writer:
        for ep_idx in sorted(merged_episodes_stats.keys()):
            writer.write({
                "episode_index": ep_idx,
                "stats": serialize_stats(merged_episodes_stats[ep_idx])
            })
    
    # 10. èšåˆç»Ÿè®¡ä¿¡æ¯
    try:
        from lerobot.datasets.compute_stats import aggregate_stats
        import numpy as np
        
        # å°† stats ä» list è½¬æ¢å› numpy array
        def convert_to_numpy(stats):
            """é€’å½’åœ°å°† list è½¬æ¢ä¸º numpy array"""
            if isinstance(stats, dict):
                return {k: convert_to_numpy(v) for k, v in stats.items()}
            elif isinstance(stats, list):
                return np.array(stats)
            return stats
        
        # è½¬æ¢æ‰€æœ‰ episodes_stats ä¸º numpy æ ¼å¼
        numpy_episodes_stats = [
            convert_to_numpy(ep_stats) 
            for ep_stats in merged_episodes_stats.values()
        ]
        
        # èšåˆç»Ÿè®¡ä¿¡æ¯
        merged_stats = aggregate_stats(numpy_episodes_stats)
        with open(target_path / "meta" / "stats.json", "w") as f:
            json.dump(serialize_stats(merged_stats), f, indent=2)
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥ aggregate_statsï¼Œè·³è¿‡ stats.json ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆ stats.json æ—¶å‡ºé”™: {e}")
        print("âš ï¸  è·³è¿‡ stats.json ç”Ÿæˆï¼Œä½†æ•°æ®é›†ä»ç„¶å¯ç”¨")
    
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼")
    print(f"  æ€» episodes: {total_episodes}")
    print(f"  æ€» frames: {total_frames}")
    print(f"  æ€» tasks: {len(merged_tasks)}")
    print(f"  ä¿å­˜è·¯å¾„: {target_path}")
    
    return target_path


def verify_merged_dataset(dataset_path: str, check_episodes: list[int] = None):
    """éªŒè¯åˆå¹¶åçš„æ•°æ®é›†ç´¢å¼•æ˜¯å¦æ­£ç¡®"""
    print(f"\nğŸ” éªŒè¯æ•°æ®é›†: {dataset_path}")
    ds = LeRobotDataset(dataset_path)
    
    if check_episodes is None:
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªã€æœ€åä¸€ä¸ªå’Œä¸­é—´çš„å‡ ä¸ª episode
        check_episodes = [0, ds.meta.total_episodes // 2, ds.meta.total_episodes - 1]
    
    for ep_idx in check_episodes:
        # è¯»å– parquet æ–‡ä»¶
        data_file = ds.root / ds.meta.get_data_file_path(ep_idx)
        df = pd.read_parquet(data_file)
        
        # æ£€æŸ¥ç´¢å¼•
        ep_index_values = df['episode_index'].unique()
        index_min = df['index'].min()
        index_max = df['index'].max()
        
        print(f"\n  Episode {ep_idx}:")
        print(f"    Data file: {data_file.name}")
        print(f"    Episode index in data: {ep_index_values}")
        print(f"    Global index range: [{index_min}, {index_max}]")
        print(f"    Frames: {len(df)}")
        
        # éªŒè¯
        assert len(ep_index_values) == 1 and ep_index_values[0] == ep_idx, \
            f"Episode index ä¸åŒ¹é…ï¼æœŸæœ› {ep_idx}ï¼Œå®é™… {ep_index_values}"
        
        # æ£€æŸ¥ index è¿ç»­æ€§
        expected_indices = list(range(index_min, index_max + 1))
        actual_indices = df['index'].tolist()
        assert actual_indices == expected_indices, \
            f"Index ä¸è¿ç»­ï¼"
        
        print(f"    âœ… éªŒè¯é€šè¿‡")
    
    print(f"\nâœ… æ•°æ®é›†éªŒè¯å®Œæˆï¼")


def push_merged_dataset(target_path: str, repo_id: str):
    """æ¨é€åˆå¹¶åçš„æ•°æ®é›†åˆ° Hub"""
    print(f"\nğŸ“¤ æ¨é€åˆ° Hub: {repo_id}")
    merged_ds = LeRobotDataset(repo_id, root=target_path)
    merged_ds.push_to_hub(
        tags=["robotics", "merged-dataset"],
        license="apache-2.0"
    )
    print("âœ… æ¨é€å®Œæˆï¼")


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    source_datasets = [
        "/home/dudu/.cache/huggingface/lerobot/zhengzi/lerobot_second_0",
        "/home/dudu/.cache/huggingface/lerobot/zhengzi/lerobot_second_1",
    ]
    
    target_repo = "zhengzi/lerobot_second_train"
    target_root = "/home/dudu/.cache/huggingface/lerobot/zhengzi/lerobot_second_train"
    
    # åˆå¹¶æ•°æ®é›†
    merged_path = merge_lerobot_datasets(
        source_paths=source_datasets,
        target_repo_id=target_repo,
        target_root=target_root
    )
    
    # éªŒè¯åˆå¹¶ç»“æœ
    verify_merged_dataset(
        str(merged_path),
        check_episodes=[0, 26, 27, 67]  # æ£€æŸ¥å…³é”®çš„ episodes
    )
    
    # æ¨é€åˆ° Hubï¼ˆå¯é€‰ï¼‰
    push_choice = input("\næ˜¯å¦æ¨é€åˆ° HuggingFace Hub? (y/n): ")
    if push_choice.lower() == 'y':
        push_merged_dataset(str(merged_path), target_repo)
